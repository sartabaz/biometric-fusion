{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iXN-I4ARAAfW",
        "og82FceSiBHl",
        "tXo9ayssiKJ6",
        "sveFP4HviUVE",
        "7Vgz6wfbizxO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sartabaz/biometric-fusion/blob/main/Fingerprint_Feature_Extraction_using_EfficientNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clear all the outputs"
      ],
      "metadata": {
        "id": "lFjOI9SNf1qK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load Libraries\n",
        "\n",
        "This section imports all necessary libraries for building and evaluating the EfficientNETV2S deep learning models. The imports are grouped logically for better understanding."
      ],
      "metadata": {
        "id": "iXN-I4ARAAfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **TensorFlow/Keras**:\n",
        "   - Core framework for building neural networks\n",
        "   - `applications` provides pre-trained models (ResNet, VGG, etc.)\n",
        "   - `ModelCheckpoint` helps save model weights during training\n",
        "\n",
        "2. **Data Handling**:\n",
        "   - Pandas for structured data operations\n",
        "   - NumPy for numerical computations and array operations\n",
        "\n",
        "3. **Visualization**:\n",
        "   - Matplotlib for basic plots (accuracy/loss curves)\n",
        "   - Seaborn for more sophisticated statistical visualizations\n",
        "\n",
        "4. **Evaluation Metrics**:\n",
        "   - ROC/AUC for binary classification performance\n",
        "   - Classification report for precision/recall metrics\n",
        "   - LabelEncoder for preparing categorical targets\n",
        "\n",
        "5. **Similarity Metrics**:\n",
        "   - Cosine distance for comparing feature vectors (you can use other distances)\n",
        "\n",
        "### Best Practices:\n",
        "- Keep imports organized by functionality\n",
        "- Only import what you need to maintain clean namespace\n",
        "- For Colab, you may need to `!pip install` certain packages first"
      ],
      "metadata": {
        "id": "cAib12UDA_9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core deep learning frameworks\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, applications  # Keras API for model building\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint  # For saving models during training\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import pandas as pd  # Dataframes and CSV handling\n",
        "import numpy as np   # Numerical operations\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt  # Basic plotting\n",
        "import seaborn as sns  # Enhanced visualizations\n",
        "\n",
        "# Model evaluation metrics\n",
        "from sklearn.metrics import roc_curve, auc, classification_report  # Classification metrics\n",
        "from sklearn.preprocessing import LabelEncoder  # For label preprocessing\n",
        "\n",
        "# Similarity metrics\n",
        "from scipy.spatial.distance import cosine  # Cosine similarity/distance calculations"
      ],
      "metadata": {
        "id": "kJerx83R_-Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Experiment Configuration\n",
        "\n",
        "This section defines the key parameters that control the model training process. These hyperparameters should be carefully tuned based on your specific dataset and hardware constraints."
      ],
      "metadata": {
        "id": "1ehglpivzyPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Core Parameters\n",
        "'''These parameters control the fundamental aspects of model training and data processing.\n",
        "python'''\n",
        "\n",
        "# Data Configuration\n",
        "NUM_CLASSES = 140\n",
        "SAMPLES_PER_CLASS = 12\n",
        "\n",
        "# Image Processing\n",
        "IMG_SIZE = 96  # Width and height of input images (square dimensions)\n",
        "                # Common sizes: 128x128 for quick experiments, 224x224/256x256 for pretrained models\n",
        "CHANNELS = 3              # Number of color channels (3 for RGB, 1 for grayscale)\n",
        "RESCALE = 1./255          # Normalization factor (scale pixel values to 0-1)\n",
        "\n",
        "# Training Process\n",
        "BATCH_SIZE = 16           # Samples per gradient update (typically 8-64)\n",
        "EPOCHS = 33               # Training iterations (monitor for overfitting)\n",
        "VALIDATION_SPLIT = 0.2    # Fraction of data reserved for validation\n",
        "SEED = 42                 # Random seed for reproducibility\n",
        "\n",
        "# Model Configuration\n",
        "BASE_LEARNING_RATE = 1e-4 # Initial learning rate for optimizer\n",
        "DROPOUT_RATE = 0.3        # Regularization strength (0-1)\n",
        "\n",
        "# Test configuration\n",
        "NUM_PAIRS=30000\n",
        "\n",
        "# Path Configuration\n",
        "CSV_PATH = 'fingerprint.csv'  # Update this path to your actual data location\n",
        "MODEL_SAVE_PATH = '/models/checkpoint.keras'  # For ModelCheckpoint\n",
        "PERFORMANCE_SAVE_PATH = 'performance_summary_Model_1.csv'\n",
        "FEATURES_SAVE_PATH = 'Palm_features.csv'"
      ],
      "metadata": {
        "id": "D4DMZuFvOF6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BLgRsW_g9Fiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Define functions\n",
        "\n",
        "A concise summary of all the functions, organized by purpose and key details:"
      ],
      "metadata": {
        "id": "MrE_ZwLxCseh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **1. Data Loading & Preprocessing**\n",
        "#### `load_data(csv_path, num_classes, num_samples)`\n",
        "- **Purpose**: Load and preprocess image data from CSV for deep learning.\n",
        "- **Inputs**:\n",
        "  - CSV path, number of classes, samples per class\n",
        "- **Outputs**:\n",
        "  - Image tensors, one-hot labels, integer labels, class names\n",
        "\n",
        "### **2. Model Construction**\n",
        "#### `create_model(num_classes)`\n",
        "- **Purpose**: Build an EfficientNetV2 model for classification.\n",
        "- **Architecture**:\n",
        "  - **Base**: Pretrained EfficientNetV2S (fine-tuning enabled)\n",
        "  - **Head**: Global pooling → dropout → softmax dense layer\n",
        "- **Config**:\n",
        "  - Input shape: `(IMG_SIZE, IMG_SIZE, 3)`\n",
        "  - Optimizer: Adam (`lr=BASE_LEARNING_RATE`, gradient clipping)\n",
        "  - Loss: Categorical crossentropy\n",
        "- **Output**: Compiled Keras model\n",
        "\n",
        "### **3. Training Visualization**\n",
        "#### `plot_training_history(history, save_path)`\n",
        "- **Purpose**: Plot training/validation metrics over epochs.\n",
        "- **Plots**:\n",
        "  1. **Accuracy**: Train vs validation\n",
        "  2. **Loss**: Train vs validation\n",
        "- **Output**: Saves figure to `save_path` and displays it.\n",
        "\n",
        "### **4. Verification Metrics**\n",
        "#### `compute_verification_metrics(features, labels, num_pairs)`\n",
        "- **Purpose**: Compute similarity scores for genuine/impostor pairs.\n",
        "- **Outputs**:\n",
        "  - `genuine_scores`: Similarities of same-class pairs\n",
        "  - `impostor_scores`: Similarities of different-class pairs\n",
        "\n",
        "### **5. Performance Evaluation**\n",
        "#### `plot_verification_metrics(genuine_scores, impostor_scores, data)`\n",
        "- **Purpose**: Visualize verification performance metrics.\n",
        "- **Plots**:\n",
        "  1. **ROC Curve**: TPR vs FPR (with AUC)\n",
        "  2. **FAR/FRR Curve**: Error rates vs threshold\n",
        "  3. **Score Distributions**: KDE of genuine/impostor scores\n",
        "  4. **DET Curve**: FAR vs FRR (log scale)\n",
        "- **Key Metric**:\n",
        "  - **EER (Equal Error Rate)**: Threshold where FAR = FRR\n",
        "- **Output**: Saves figure as `verification_metrics_{data}.png`.\n",
        "\n",
        "### **Helper Functions**\n",
        "#### `compute_far_frr(genuine_scores, impostor_scores)`\n",
        "- **Purpose**: Calculate False Acceptance/Rejection Rates.\n",
        "- **Outputs**:\n",
        "  - `far`, `frr`, `thresholds` arrays for plotting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Dependencies**\n",
        "- **Libraries**: TensorFlow, NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn\n",
        "- **Hardware**: GPU-accelerated (via TensorFlow) recommended."
      ],
      "metadata": {
        "id": "n6Rr3-ifGGOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data from CSV\n",
        "\n",
        "def load_data(csv_path, num_classes, num_samples):\n",
        "  \"\"\"\n",
        "  Load and preprocess data from CSV file for deep learning model.\n",
        "\n",
        "  Args:\n",
        "      csv_path (str): Path to CSV file containing image data and labels\n",
        "      num_classes (int): Number of unique classes in dataset\n",
        "      num_samples (int): Number of samples per class to load\n",
        "\n",
        "  Returns:\n",
        "      tuple: (X_tensor, y_tensor_categorical, y_tensor_encoded, class_names)\n",
        "      - X_tensor: Tensor of preprocessed images (shape: [num_samples, IMG_SIZE, IMG_SIZE, 3])\n",
        "      - y_tensor_categorical: One-hot encoded labels\n",
        "      - y_tensor_encoded: Integer encoded labels\n",
        "      - class_names: List of original class names\n",
        "\n",
        "  Processing Steps:\n",
        "  1. Load CSV data with pandas\n",
        "  2. Extract labels and image pixel values\n",
        "  3. Reshape flat pixel arrays to 2D images\n",
        "  4. Convert grayscale to RGB by channel duplication\n",
        "  5. Normalize pixel values to [0,1] range\n",
        "  6. Encode labels (integer and one-hot)\n",
        "  \"\"\"\n",
        "  # Load CSV data\n",
        "  # Load CSV data with error handling\n",
        "\n",
        "\n",
        "  try:\n",
        "      df = pd.read_csv(csv_path, header=None, nrows=num_classes * num_samples)\n",
        "  except FileNotFoundError:\n",
        "      raise ValueError(f\"CSV file not found at specified path: {csv_path}\")\n",
        "\n",
        "  # Extract labels\n",
        "  y = df.iloc[:, 0].values\n",
        "\n",
        "  # Extract and preprocess images\n",
        "  X = df.iloc[:, 1:IMG_SIZE*IMG_SIZE+1].values.astype('float32')\n",
        "\n",
        "  # Reshape to 2D grayscale images\n",
        "  X = X.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "\n",
        "  # Convert grayscale to RGB by repeating across 3 channels\n",
        "  X = np.repeat(X, 3, axis=-1)\n",
        "\n",
        "  # Normalize pixel values\n",
        "  X = X * RESCALE\n",
        "\n",
        "  # Convert to TensorFlow tensors\n",
        "  X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "  y_tensor = tf.convert_to_tensor(y, dtype=tf.int32)\n",
        "\n",
        "  # Encode labels\n",
        "  le = LabelEncoder()\n",
        "  y_encoded = le.fit_transform(y)\n",
        "  y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes=num_classes)\n",
        "\n",
        "  # Convert label arrays to tensors\n",
        "  y_tensor_encoded = tf.convert_to_tensor(y_encoded, dtype=tf.int32)\n",
        "  y_tensor_categorical = tf.convert_to_tensor(y_categorical, dtype=tf.float32)\n",
        "\n",
        "  return X_tensor, y_tensor_categorical, y_tensor_encoded, le.classes_\n",
        "\n",
        "# Build EfficientNetV2 model\n",
        "def create_model(num_classes):\n",
        "\n",
        "  ''' Create EfficientNetV2 model with transfer learning.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of output classes\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled model ready for training\n",
        "\n",
        "    Architecture:\n",
        "    1. EfficientNetV2S base (pretrained on ImageNet)\n",
        "    2. Global average pooling\n",
        "    3. Dropout layer for regularization\n",
        "    4. Dense output layer with softmax activation\n",
        "\n",
        "    Note:\n",
        "    - Base model is set as trainable for fine-tuning\n",
        "    - Adam optimizer with gradient clipping\n",
        "    - Learning rate 1e-5 is good starting point for fine-tuning'''\n",
        "  base_model = applications.EfficientNetV2S(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, CHANNELS),\n",
        "        pooling='avg'\n",
        "    )\n",
        "\n",
        "  # Freeze base model\n",
        "  base_model.trainable = True\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
        "  x = base_model(inputs, training=False)\n",
        "  x = layers.Dropout(DROPOUT_RATE)(x)\n",
        "  outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=BASE_LEARNING_RATE,\n",
        "                                           clipnorm=1.0\n",
        "                                          ),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "# Plot training history\n",
        "def plot_training_history(history):\n",
        "  \"\"\"\n",
        "    Plot training and validation metrics over epochs.\n",
        "\n",
        "    Args:\n",
        "        history: Keras History object returned from model.fit()\n",
        "        save_path: Where to save the generated plot\n",
        "\n",
        "    Generates:\n",
        "        1. Accuracy plot (train vs validation)\n",
        "        2. Loss plot (train vs validation)\n",
        "    \"\"\"\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "  # Accuracy plot\n",
        "  ax[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
        "  ax[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "  ax[0].set_title('Model Accuracy')\n",
        "  ax[0].set_ylabel('Accuracy')\n",
        "  ax[0].set_xlabel('Epoch')\n",
        "  ax[0].legend()\n",
        "\n",
        "  # Loss plot\n",
        "  ax[1].plot(history.history['loss'], label='Train Loss')\n",
        "  ax[1].plot(history.history['val_loss'], label='Validation Loss')\n",
        "  ax[1].set_title('Model Loss')\n",
        "  ax[1].set_ylabel('Loss')\n",
        "  ax[1].set_xlabel('Epoch')\n",
        "  ax[1].legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('training_history.png')\n",
        "  plt.show()\n",
        "\n",
        "# Compute verification metrics\n",
        "def compute_verification_metrics(features, labels, num_pairs=1000):\n",
        "  \"\"\"\n",
        "    Compute genuine and impostor similarity scores for verification.\n",
        "\n",
        "    Args:\n",
        "        features: Embeddings from model (n_samples, feature_dim)\n",
        "        labels: Corresponding class labels\n",
        "        num_pairs: Number of pairs to generate\n",
        "\n",
        "    Returns:\n",
        "        tuple: (genuine_scores, impostor_scores)\n",
        "    \"\"\"\n",
        "  # Generate genuine and impostor pairs\n",
        "  genuine_pairs = []\n",
        "  impostor_pairs = []\n",
        "\n",
        "  # Group indices by class\n",
        "  class_indices = {}\n",
        "  for i, label in enumerate(labels):\n",
        "      label_int = label.numpy()\n",
        "      if label_int not in class_indices:\n",
        "          class_indices[label_int] = []\n",
        "      class_indices[label_int].append(i)\n",
        "\n",
        "  # Create genuine pairs (same class)\n",
        "  for label, indices in class_indices.items():\n",
        "      if len(indices) < 2:\n",
        "          continue\n",
        "      np.random.shuffle(indices)\n",
        "      for i in range(0, len(indices) - 1, 2):\n",
        "          if len(genuine_pairs) < num_pairs // 2:\n",
        "              genuine_pairs.append((indices[i], indices[i+1]))\n",
        "\n",
        "  # Create impostor pairs (different classes)\n",
        "  class_list = list(class_indices.keys())\n",
        "  while len(impostor_pairs) < num_pairs // 2:\n",
        "      class1, class2 = np.random.choice(class_list, 2, replace=False)\n",
        "      if class1 == class2 or not class_indices[class1] or not class_indices[class2]:\n",
        "          continue\n",
        "      idx1 = np.random.choice(class_indices[class1])\n",
        "      idx2 = np.random.choice(class_indices[class2])\n",
        "      impostor_pairs.append((idx1, idx2))\n",
        "\n",
        "  # Compute similarities\n",
        "  genuine_scores = []\n",
        "  for i, j in genuine_pairs:\n",
        "      feat1 = features[i]\n",
        "      feat2 = features[j]\n",
        "      similarity = 1 - cosine(feat1, feat2)\n",
        "      genuine_scores.append(similarity)\n",
        "\n",
        "  impostor_scores = []\n",
        "  for i, j in impostor_pairs:\n",
        "      feat1 = features[i]\n",
        "      feat2 = features[j]\n",
        "      similarity = 1 - cosine(feat1, feat2)\n",
        "      impostor_scores.append(similarity)\n",
        "\n",
        "  return np.array(genuine_scores), np.array(impostor_scores)\n",
        "\n",
        "# Compute FAR and FRR\n",
        "def compute_far_frr(genuine_scores, impostor_scores):\n",
        "    thresholds = np.linspace(0, 1, 100)\n",
        "    far = np.zeros_like(thresholds)\n",
        "    frr = np.zeros_like(thresholds)\n",
        "\n",
        "    for i, thresh in enumerate(thresholds):\n",
        "        # False Acceptance Rate\n",
        "        far[i] = np.sum(impostor_scores >= thresh) / len(impostor_scores)\n",
        "\n",
        "        # False Rejection Rate\n",
        "        frr[i] = np.sum(genuine_scores < thresh) / len(genuine_scores)\n",
        "\n",
        "    return far, frr, thresholds\n",
        "\n",
        "# Plot ROC and FAR/FRR curves\n",
        "def plot_verification_metrics(genuine_scores, impostor_scores,data='NIR'):\n",
        "  \"\"\"\n",
        "    Generate comprehensive verification performance plots.\n",
        "\n",
        "    Args:\n",
        "        genuine_scores: Similarity scores for genuine pairs\n",
        "        impostor_scores: Similarity scores for impostor pairs\n",
        "        data: Dataset identifier for plot titles\n",
        "\n",
        "    Returns:\n",
        "        tuple: (eer, eer_threshold)\n",
        "    \"\"\"\n",
        "  far, frr, thresholds = compute_far_frr(genuine_scores, impostor_scores)\n",
        "\n",
        "  # Compute ROC curve\n",
        "  y_true = np.concatenate([np.ones_like(genuine_scores), np.zeros_like(impostor_scores)])\n",
        "  y_score = np.concatenate([genuine_scores, impostor_scores])\n",
        "  fpr, tpr, roc_thresholds = roc_curve(y_true, y_score)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  # Find EER (Equal Error Rate)\n",
        "  eer_idx = np.argmin(np.abs(far - frr))\n",
        "  eer = (far[eer_idx] + frr[eer_idx]) / 2\n",
        "  eer_thresh = thresholds[eer_idx]\n",
        "\n",
        "  # Create plots\n",
        "  plt.figure(figsize=(15, 10))\n",
        "\n",
        "  # ROC curve\n",
        "  plt.subplot(2, 2, 1)\n",
        "  plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "  plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate (FAR)')\n",
        "  plt.ylabel('True Positive Rate (GAR)')\n",
        "  plt.title('Receiver Operating Characteristic')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  # FAR/FRR curve\n",
        "  plt.subplot(2, 2, 2)\n",
        "  plt.plot(thresholds, far, 'b-', label='FAR')\n",
        "  plt.plot(thresholds, frr, 'r-', label='FRR')\n",
        "  plt.axvline(x=eer_thresh, color='g', linestyle='--', label=f'EER Threshold ({eer_thresh:.2f})')\n",
        "  plt.xlabel('Similarity Threshold')\n",
        "  plt.ylabel('Error Rate')\n",
        "  plt.title(f'FAR/FRR Curve (EER = {eer:.4f})')\n",
        "  plt.legend()\n",
        "\n",
        "  # Score distributions\n",
        "  plt.subplot(2, 2, 3)\n",
        "  sns.kdeplot(genuine_scores, label='Genuine Scores', fill=True)\n",
        "  sns.kdeplot(impostor_scores, label='Impostor Scores', fill=True)\n",
        "  plt.axvline(x=eer_thresh, color='g', linestyle='--', label=f'EER Threshold')\n",
        "  plt.xlabel('Similarity Score')\n",
        "  plt.ylabel('Density')\n",
        "  plt.title('Score Distributions')\n",
        "  plt.legend()\n",
        "\n",
        "  # Detection Error Tradeoff (DET)\n",
        "  plt.subplot(2, 2, 4)\n",
        "  plt.plot(far, frr)\n",
        "  plt.scatter(far[eer_idx], frr[eer_idx], color='red', zorder=10,\n",
        "              label=f'EER ({eer:.4f})')\n",
        "  plt.xscale('log')\n",
        "  plt.yscale('log')\n",
        "  plt.xlabel('False Acceptance Rate (FAR)')\n",
        "  plt.ylabel('False Rejection Rate (FRR)')\n",
        "  plt.title('Detection Error Tradeoff (DET) Curve')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('verification_metrics_'+data+'.png')\n",
        "  plt.show()\n",
        "\n",
        "  return eer, eer_thresh"
      ],
      "metadata": {
        "id": "JPzwIzzMOMkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data_by_indices(X, y_categorical, y_encoded, total, n_train):\n",
        "    \"\"\"\n",
        "    Splits data tensors into training, validation, and test sets based on pre-defined index logic,\n",
        "    with shuffling by class.\n",
        "\n",
        "    Args:\n",
        "        X: TensorFlow tensor containing image data.\n",
        "        y_categorical: TensorFlow tensor containing one-hot encoded labels.\n",
        "        y_encoded: TensorFlow tensor containing integer encoded labels.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing: (X_train, y_train, X_val, y_val, X_test, y_test, y_test_encoded)\n",
        "    \"\"\"\n",
        "    train_indices = []\n",
        "    val_indices = []\n",
        "    test_indices = []\n",
        "\n",
        "    total_samples = tf.shape(X)[0].numpy()\n",
        "\n",
        "    # Group indices by class\n",
        "    class_indices = {}\n",
        "    for i, label in enumerate(y_encoded.numpy()): # Use numpy() to iterate over tensor\n",
        "        if label not in class_indices:\n",
        "            class_indices[label] = []\n",
        "        class_indices[label].append(i)\n",
        "\n",
        "    # Shuffle indices within each class\n",
        "    for label, indices in class_indices.items():\n",
        "        np.random.shuffle(indices)\n",
        "        class_indices[label] = indices # Update the shuffled indices\n",
        "\n",
        "    # Distribute shuffled indices to train, val, and test sets\n",
        "    for label, indices in class_indices.items():\n",
        "        # Calculate the number of samples for this class\n",
        "        num_class_samples = len(indices)\n",
        "\n",
        "        # Calculate the split points for this class\n",
        "        num_train = n_train\n",
        "        num_val = (total - n_train) // 2\n",
        "        num_test = (total - n_train) // 2\n",
        "\n",
        "        # Ensure split points don't exceed available samples for this class\n",
        "        num_train = min(num_train, num_class_samples)\n",
        "        num_val = min(num_val, num_class_samples - num_train)\n",
        "        num_test = min(num_test, num_class_samples - num_train - num_val)\n",
        "\n",
        "\n",
        "        # Distribute the shuffled indices\n",
        "        train_indices.extend(indices[:num_train])\n",
        "        val_indices.extend(indices[num_train : num_train + num_val])\n",
        "        test_indices.extend(indices[num_train + num_val : num_train + num_val + num_test])\n",
        "\n",
        "    # Convert indices lists to TensorFlow tensors\n",
        "    train_indices_tensor = tf.convert_to_tensor(train_indices, dtype=tf.int32)\n",
        "    val_indices_tensor = tf.convert_to_tensor(val_indices, dtype=tf.int32)\n",
        "    test_indices_tensor = tf.convert_to_tensor(test_indices, dtype=tf.int32)\n",
        "\n",
        "    # Use tf.gather to split the data tensors\n",
        "    X_train = tf.gather(X, train_indices_tensor)\n",
        "    y_train = tf.gather(y_categorical, train_indices_tensor)\n",
        "\n",
        "    X_val = tf.gather(X, val_indices_tensor)\n",
        "    y_val = tf.gather(y_categorical, val_indices_tensor)\n",
        "\n",
        "    X_test = tf.gather(X, test_indices_tensor)\n",
        "    y_test = tf.gather(y_categorical, test_indices_tensor)\n",
        "    y_test_encoded = tf.gather(y_encoded, test_indices_tensor)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, y_test_encoded"
      ],
      "metadata": {
        "id": "68fbwSq7CgG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Start The process"
      ],
      "metadata": {
        "id": "j4hJY76UWfdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Key Workflow Summary**\n",
        "1. **Data Prep**:  \n",
        "   `load_data()` → Preprocessed tensors  \n",
        "2. **Model Training**:  \n",
        "   `create_model()` → Train → `plot_training_history()`  \n",
        "3. **Evaluation**:  \n",
        "   Extract features → `compute_verification_metrics()` → `plot_verification_metrics()`"
      ],
      "metadata": {
        "id": "Vm3wpce-GM0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "X, y_categorical, y_encoded, class_names = load_data(CSV_PATH,NUM_CLASSES,SAMPLES_PER_CLASS)\n",
        "NUM_CLASSES = len(class_names)"
      ],
      "metadata": {
        "id": "T2cxBDACOUHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# Convert the integer encoded labels to a list or numpy array\n",
        "y_encoded_list = y_encoded.numpy()\n",
        "# Use collections.Counter to count samples per class\n",
        "class_counts = Counter(y_encoded_list)"
      ],
      "metadata": {
        "id": "E8ERv9XYD61O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "# Call the function to get the split data tensors\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, y_test_encoded = split_data_by_indices(X, y_categorical, y_encoded,class_counts[0],8)"
      ],
      "metadata": {
        "id": "ht01F699Pkdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES"
      ],
      "metadata": {
        "id": "qIprxZFO9VoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Use for test on the whole dataset**"
      ],
      "metadata": {
        "id": "ExZSpAwApE5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X\n",
        "y_test = y_categorical\n",
        "y_test_encoded = y_encoded"
      ],
      "metadata": {
        "id": "VR9QUieJpQMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Use if data augmentation is needed**\n"
      ],
      "metadata": {
        "id": "tR6oW1FDatKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")"
      ],
      "metadata": {
        "id": "ohtSysIEOfs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Skip and load a trained model**"
      ],
      "metadata": {
        "id": "8U7D9fyBbpHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Create the model and train\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H8tT1jyYe9Ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(NUM_CLASSES)\n",
        "model.summary()\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "        filepath=MODEL_SAVE_PATH, # Update the path\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "  train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "  #X_train,y_train,\n",
        "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=3),\n",
        "        checkpoint_callback\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "NS-5y88_OiTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Continue training if necessary"
      ],
      "metadata": {
        "id": "vuyGhl5VOWNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the previously saved model\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
        "\n",
        "# Compile the model (use the same configuration as before)\n",
        "loaded_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5,\n",
        "                                       clipnorm=1.0\n",
        "                                      ),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Continue training for additional epochs\n",
        "additional_epochs = 10 # Define how many more epochs you want to train\n",
        "history_continued = loaded_model.fit(\n",
        "    X_train, y_train,\n",
        "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS + additional_epochs,  # Continue from the previous total epochs\n",
        "    initial_epoch=EPOCHS, # Start training from the epoch where the previous training stopped\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=3),\n",
        "        # You might want to use a new ModelCheckpoint to save the best model from this continued training\n",
        "        ModelCheckpoint(\n",
        "            filepath=MODEL_SAVE_PATH,\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            mode='max',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "XSDyt--_Oqnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the history dictionary\n",
        "for key in history_continued.history:\n",
        "    history.history[key].extend(history_continued.history[key])\n",
        "\n",
        "# Plot the updated training history\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "_ZWYydD1QdmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. Plot the training history:\n",
        "\n",
        "\n",
        "1.   Check accuracy and loss\n",
        "2.   Check underfitting or overfitting\n",
        "\n"
      ],
      "metadata": {
        "id": "jkJdMuBtfFBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "Fvd7iUd_vKQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load saved best model**"
      ],
      "metadata": {
        "id": "ZZWGLM17vP1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model = tf.keras.models.load_model(MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "0o3KAm-vvOB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Evaluate the model on test data"
      ],
      "metadata": {
        "id": "og82FceSiBHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "OMFrSfgLw90q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Predict and generate performances"
      ],
      "metadata": {
        "id": "tXo9ayssiKJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test_encoded, y_pred_classes, target_names=class_names, output_dict=True)\n",
        "\n",
        "# Extract macro and micro averages\n",
        "macro_avg = report['macro avg']\n",
        "micro_avg = report['weighted avg'] # sklearn's weighted avg is often referred to as micro avg in some contexts\n",
        "\n",
        "# Create a pandas DataFrame for the summary table\n",
        "performance_summary = pd.DataFrame({\n",
        "    'Metric': ['Precision', 'Recall', 'F1-Score'],\n",
        "    'Macro Average': [macro_avg['precision'], macro_avg['recall'], macro_avg['f1-score']],\n",
        "    'Micro Average': [micro_avg['precision'], micro_avg['recall'], micro_avg['f1-score']]\n",
        "})\n",
        "\n",
        "print(\"\\nPerformance Summary (Macro and Micro Averages):\")\n",
        "performance_summary\n"
      ],
      "metadata": {
        "id": "eLhYyMRp3q0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Save performance summary"
      ],
      "metadata": {
        "id": "sveFP4HviUVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "performance_summary.to_csv(PERFORMANCE_SAVE_PATH, index=False)\n",
        "print(\"Performance summary table saved to csv file\")"
      ],
      "metadata": {
        "id": "6Vw3YY8n9A1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Get the feature extractor from the model"
      ],
      "metadata": {
        "id": "7Vgz6wfbizxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature extractor\n",
        "feature_extractor = tf.keras.Model(\n",
        "    inputs=model.input,\n",
        "    outputs=model.layers[-2].output\n",
        ")"
      ],
      "metadata": {
        "id": "PiJXD2d1Ok5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Extract features from the test data"
      ],
      "metadata": {
        "id": "xS0nwKJJjAVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from test set\n",
        "val_features = feature_extractor.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "PfG3IpTmOqhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Compute metrics and plot them"
      ],
      "metadata": {
        "id": "ficcvOp4jHV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute verification metrics\n",
        "genuine_scores, impostor_scores = compute_verification_metrics(\n",
        "    val_features,\n",
        "    y_test_encoded,\n",
        "    num_pairs=NUM_PAIRS\n",
        ")\n",
        "\n",
        "# Plot verification metrics\n",
        "eer, eer_threshold = plot_verification_metrics(genuine_scores, impostor_scores)\n",
        "print(f\"Equal Error Rate (EER): {eer:.4f}\")\n",
        "print(f\"Optimal Threshold: {eer_threshold:.4f}\")"
      ],
      "metadata": {
        "id": "oIzWi-xQjF-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. Get a subset of classes"
      ],
      "metadata": {
        "id": "uKhz1IzTjm7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_subset = tf.gather(X, tf.range(140 * 12))\n",
        "y_subset = tf.gather(y_encoded, tf.range(140 * 12))\n",
        "print(f\"Shape of the 140*12 samples from X: {X_subset.shape}\")"
      ],
      "metadata": {
        "id": "Djf5_gKknHFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d. Extract their features"
      ],
      "metadata": {
        "id": "gt9jTS3wjuke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_features = feature_extractor.predict(X_subset, batch_size=BATCH_SIZE, verbose=1)"
      ],
      "metadata": {
        "id": "jjAvaUPunIXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## e. Save them"
      ],
      "metadata": {
        "id": "ai66YvjPjztO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save val_features in csv\n",
        "features_df = pd.DataFrame(val_features)\n",
        "\n",
        "features_df['class']=y_subset.numpy()\n",
        "features_df.to_csv(FEATURES_SAVE_PATH, index=False)\n",
        "print(\"features saved to val_features.csv\")"
      ],
      "metadata": {
        "id": "MpYhLOv3n4Vd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}