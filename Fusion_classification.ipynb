{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sartabaz/biometric-fusion/blob/main/Fusion_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load Libraries\n",
        "\n",
        "This section imports all necessary libraries for evaluating the EfficientNETV2S deep learning models on fused palmprint and fingerprint. The imports are grouped logically for better understanding."
      ],
      "metadata": {
        "id": "CIP_PnX7dT9x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcRcObHgOBpN"
      },
      "outputs": [],
      "source": [
        "# Core deep learning frameworks\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, applications  # Keras API for model building\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import pandas as pd  # Dataframes and CSV handling\n",
        "import numpy as np   # Numerical operations\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt  # Basic plotting\n",
        "import seaborn as sns  # Enhanced visualizations\n",
        "\n",
        "# Model evaluation metrics\n",
        "from sklearn.metrics import roc_curve, auc, classification_report  # Classification metrics\n",
        "from sklearn.preprocessing import LabelEncoder  # For label preprocessing\n",
        "\n",
        "# Similarity metrics\n",
        "from scipy.spatial.distance import cosine  # Cosine similarity/distance calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **TensorFlow/Keras**:\n",
        "   - Core framework for building neural networks\n",
        "   - `applications` provides pre-trained models (ResNet, VGG, etc.)\n",
        "\n",
        "2. **Data Handling**:\n",
        "   - Pandas for structured data operations\n",
        "   - NumPy for numerical computations and array operations\n",
        "\n",
        "3. **Visualization**:\n",
        "   - Matplotlib for basic plots (accuracy/loss curves)\n",
        "   - Seaborn for more sophisticated statistical visualizations\n",
        "\n",
        "4. **Evaluation Metrics**:\n",
        "   - ROC/AUC for binary classification performance\n",
        "   - Classification report for precision/recall metrics\n",
        "   - LabelEncoder for preparing categorical targets\n",
        "\n",
        "5. **Similarity Metrics**:\n",
        "   - Cosine distance for comparing feature vectors (you can use other distances)\n",
        "\n",
        "### Best Practices:\n",
        "- Keep imports organized by functionality\n",
        "- Only import what you need to maintain clean namespace\n",
        "- For Colab, you may need to `!pip install` certain packages first"
      ],
      "metadata": {
        "id": "FWYAFCqPdDXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Experiment Configuration\n",
        "\n",
        "This section defines the needed variables for processing features fusion."
      ],
      "metadata": {
        "id": "QUT3T1fbdicY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4DMZuFvOF6a"
      },
      "outputs": [],
      "source": [
        "# Core parameters\n",
        "\n",
        "# Data Configuration\n",
        "NUM_CLASSES = 140\n",
        "SAMPLES_PER_CLASS = 12\n",
        "\n",
        "# Test configuration\n",
        "NUM_PAIRS=30000\n",
        "\n",
        "# Path Configuration\n",
        "MODEL_SAVE_PATH = '/models/checkpoint.keras'  # For ModelCheckpoint\n",
        "PALM_FEATURES_PATH = 'Palm_features.csv'  # Update this path\n",
        "FINGER_FEATURES_PATH = 'finger_features.csv'  # Update this path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Define functions\n",
        "\n",
        "A concise summary of all the functions, organized by purpose and key details:"
      ],
      "metadata": {
        "id": "5YdO3oVeeOlc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPzwIzzMOMkP"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data from CSV\n",
        "\n",
        "def load_features(csv_path, num_classes, num_samples):\n",
        "    # Load CSV data\n",
        "    df = pd.read_csv(csv_path, nrows=num_classes * num_samples)\n",
        "\n",
        "    # Extract labels\n",
        "    y = df.iloc[:,-1].values\n",
        "\n",
        "    # Extract and preprocess images\n",
        "    X = df.iloc[:, :-1].values.astype('float32')\n",
        "\n",
        "    # Convert to TensorFlow tensors\n",
        "    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "    y_tensor = tf.convert_to_tensor(y, dtype=tf.int32)\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "    y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes=num_classes)\n",
        "\n",
        "    return X, y_categorical, y_encoded, le.classes_\n",
        "\n",
        "# Compute verification metrics\n",
        "def compute_verification_metrics(features, labels, num_pairs=1000):\n",
        "    # Generate genuine and impostor pairs\n",
        "    genuine_pairs = []\n",
        "    impostor_pairs = []\n",
        "\n",
        "    # Group indices by class\n",
        "    class_indices = {}\n",
        "    for i, label in enumerate(labels):\n",
        "        label_int = label.numpy()\n",
        "        if label_int not in class_indices:\n",
        "            class_indices[label_int] = []\n",
        "        class_indices[label_int].append(i)\n",
        "\n",
        "    # Create genuine pairs (same class)\n",
        "    for label, indices in class_indices.items():\n",
        "        if len(indices) < 2:\n",
        "            continue\n",
        "        np.random.shuffle(indices)\n",
        "        for i in range(0, len(indices) - 1, 2):\n",
        "            if len(genuine_pairs) < num_pairs // 2:\n",
        "                genuine_pairs.append((indices[i], indices[i+1]))\n",
        "\n",
        "    # Create impostor pairs (different classes)\n",
        "    class_list = list(class_indices.keys())\n",
        "    while len(impostor_pairs) < num_pairs // 2:\n",
        "        class1, class2 = np.random.choice(class_list, 2, replace=False)\n",
        "        if class1 == class2 or not class_indices[class1] or not class_indices[class2]:\n",
        "            continue\n",
        "        idx1 = np.random.choice(class_indices[class1])\n",
        "        idx2 = np.random.choice(class_indices[class2])\n",
        "        impostor_pairs.append((idx1, idx2))\n",
        "\n",
        "    # Compute similarities\n",
        "    genuine_scores = []\n",
        "    for i, j in genuine_pairs:\n",
        "        feat1 = features[i]\n",
        "        feat2 = features[j]\n",
        "        similarity = 1 - cosine(feat1, feat2)\n",
        "        genuine_scores.append(similarity)\n",
        "\n",
        "    impostor_scores = []\n",
        "    for i, j in impostor_pairs:\n",
        "        feat1 = features[i]\n",
        "        feat2 = features[j]\n",
        "        similarity = 1 - cosine(feat1, feat2)\n",
        "        impostor_scores.append(similarity)\n",
        "\n",
        "    return np.array(genuine_scores), np.array(impostor_scores)\n",
        "\n",
        "# Compute FAR and FRR\n",
        "def compute_far_frr(genuine_scores, impostor_scores):\n",
        "    thresholds = np.linspace(0, 1, 100)\n",
        "    far = np.zeros_like(thresholds)\n",
        "    frr = np.zeros_like(thresholds)\n",
        "\n",
        "    for i, thresh in enumerate(thresholds):\n",
        "        # False Acceptance Rate\n",
        "        far[i] = np.sum(impostor_scores >= thresh) / len(impostor_scores)\n",
        "\n",
        "        # False Rejection Rate\n",
        "        frr[i] = np.sum(genuine_scores < thresh) / len(genuine_scores)\n",
        "\n",
        "    return far, frr, thresholds\n",
        "\n",
        "# Plot ROC and FAR/FRR curves\n",
        "def plot_verification_metrics(genuine_scores, impostor_scores,data='NIR'):\n",
        "    far, frr, thresholds = compute_far_frr(genuine_scores, impostor_scores)\n",
        "\n",
        "    # Compute ROC curve\n",
        "    y_true = np.concatenate([np.ones_like(genuine_scores), np.zeros_like(impostor_scores)])\n",
        "    y_score = np.concatenate([genuine_scores, impostor_scores])\n",
        "    fpr, tpr, roc_thresholds = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Find EER (Equal Error Rate)\n",
        "    eer_idx = np.argmin(np.abs(far - frr))\n",
        "    eer = (far[eer_idx] + frr[eer_idx]) / 2\n",
        "    eer_thresh = thresholds[eer_idx]\n",
        "\n",
        "    # Create plots\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # ROC curve\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate (FAR)')\n",
        "    plt.ylabel('True Positive Rate (GAR)')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # FAR/FRR curve\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(thresholds, far, 'b-', label='FAR')\n",
        "    plt.plot(thresholds, frr, 'r-', label='FRR')\n",
        "    plt.axvline(x=eer_thresh, color='g', linestyle='--', label=f'EER Threshold ({eer_thresh:.2f})')\n",
        "    plt.xlabel('Similarity Threshold')\n",
        "    plt.ylabel('Error Rate')\n",
        "    plt.title(f'FAR/FRR Curve (EER = {eer:.4f})')\n",
        "    plt.legend()\n",
        "\n",
        "    # Score distributions\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.kdeplot(genuine_scores, label='Genuine Scores', fill=True)\n",
        "    sns.kdeplot(impostor_scores, label='Impostor Scores', fill=True)\n",
        "    plt.axvline(x=eer_thresh, color='g', linestyle='--', label=f'EER Threshold')\n",
        "    plt.xlabel('Similarity Score')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Score Distributions')\n",
        "    plt.legend()\n",
        "\n",
        "    # Detection Error Tradeoff (DET)\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(far, frr)\n",
        "    plt.scatter(far[eer_idx], frr[eer_idx], color='red', zorder=10,\n",
        "                label=f'EER ({eer:.4f})')\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('False Acceptance Rate (FAR)')\n",
        "    plt.ylabel('False Rejection Rate (FRR)')\n",
        "    plt.title('Detection Error Tradeoff (DET) Curve')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('verification_metrics_'+data+'.png')\n",
        "    plt.show()\n",
        "\n",
        "    return eer, eer_thresh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IiDCvWeOa-q"
      },
      "outputs": [],
      "source": [
        "def split_data_by_indices(X, y_categorical, y_encoded, total, n_train):\n",
        "    \"\"\"\n",
        "    Splits data tensors into training, validation, and test sets based on pre-defined index logic.\n",
        "\n",
        "    Args:\n",
        "        X: TensorFlow tensor containing image data.\n",
        "        y_categorical: TensorFlow tensor containing one-hot encoded labels.\n",
        "        y_encoded: TensorFlow tensor containing integer encoded labels.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing: (X_train, y_train, X_val, y_val, X_test, y_test, y_test_encoded)\n",
        "    \"\"\"\n",
        "    train_indices = []\n",
        "    test_indices = []\n",
        "\n",
        "    # The total number of samples is derived from the shape of the input tensors\n",
        "    total_samples = tf.shape(X)[0].numpy() # Convert tensor shape to numpy int\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while (i < total_samples):\n",
        "        # Training indices\n",
        "        while (i < j + n_train):\n",
        "            if i < total_samples: # Ensure we don't go out of bounds\n",
        "                train_indices.append(i)\n",
        "                i += 1\n",
        "            else:\n",
        "                break # Exit if we've used all samples\n",
        "        j = i + total - n_train # Advance j for the next class\n",
        "        # Test indices\n",
        "        while (i < j):\n",
        "            if i < total_samples:\n",
        "                test_indices.append(i)\n",
        "                i += 1\n",
        "            else:\n",
        "                break\n",
        "    # Convert indices lists to TensorFlow tensors\n",
        "    train_indices_tensor = tf.convert_to_tensor(train_indices, dtype=tf.int32)\n",
        "    test_indices_tensor = tf.convert_to_tensor(test_indices, dtype=tf.int32)\n",
        "\n",
        "    # Use tf.gather to split the data tensors\n",
        "    X_train = tf.gather(X, train_indices_tensor)\n",
        "    y_train = tf.gather(y_categorical, train_indices_tensor)\n",
        "\n",
        "    X_test = tf.gather(X, test_indices_tensor)\n",
        "    y_test = tf.gather(y_categorical, test_indices_tensor)\n",
        "    y_test_encoded = tf.gather(y_encoded, test_indices_tensor)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, y_test_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Apply fusion and test"
      ],
      "metadata": {
        "id": "bhHpYUSlgC4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Key Workflow Summary**\n",
        "1. **Data Prep**:  \n",
        "   `load_features()` → concatenate  \n",
        "2. **Model Training**:  \n",
        "   Train classifiers → print results\n",
        "3. **Evaluation**:  \n",
        "   Extract features → `compute_verification_metrics()` → `plot_verification_metrics()`\n"
      ],
      "metadata": {
        "id": "YGU9GbsagOid"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2cxBDACOUHq"
      },
      "outputs": [],
      "source": [
        "# Load features\n",
        "X_palm, y_categorical, y_encoded, class_names = load_features(PALM_FEATURES_PATH,NUM_CLASSES,SAMPLES_PER_CLASS)\n",
        "X_finger, y_categorical, y_encoded, class_names = load_features(FINGER_FEATURES_PATH,NUM_CLASSES,SAMPLES_PER_CLASS)\n",
        "X = np.concatenate([X_palm, X_finger], axis=-1)\n",
        "\n",
        "NUM_CLASSES = len(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# Convert the integer encoded labels to a list or numpy array\n",
        "y_encoded_list = y_encoded.numpy()\n",
        "# Use collections.Counter to count samples per class\n",
        "class_counts = Counter(y_encoded_list)"
      ],
      "metadata": {
        "id": "Avg8E_rjftt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWMczYWlHr4f"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test, y_test_encoded = split_data_by_indices(X, y_categorical, y_encoded, class_counts, 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_encoded"
      ],
      "metadata": {
        "id": "OSpSYKtnCbbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Use for test on the whole dataset**"
      ],
      "metadata": {
        "id": "xwB-5TUfgiYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = tf.concat([X_train, X_test], axis=0)\n",
        "y_test = tf.concat([y_train, y_test], axis=0)\n",
        "y_test_encoded = tf.convert_to_tensor(y_encoded, dtype=tf.int32)"
      ],
      "metadata": {
        "id": "GQ8nuD5aAfwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Train classifiers on features"
      ],
      "metadata": {
        "id": "7BnPmXgBgqCV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2diNP02mH8Fk"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert TensorFlow tensors to NumPy arrays for scikit-learn\n",
        "X_train_np = X_train.numpy()\n",
        "y_train_encoded_np = np.argmax(y_train.numpy(), axis=1) # Get the original class labels\n",
        "X_test_np = X_test.numpy()\n",
        "y_test_encoded_np = np.argmax(y_test.numpy(), axis=1) # Get the original class labels\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"SVM\": SVC(probability=True), # probability=True for potential future use with ROC curves\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate each classifier\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    clf.fit(X_train_np, y_train_encoded_np)\n",
        "    y_pred = clf.predict(X_test_np)\n",
        "    accuracy = accuracy_score(y_test_encoded_np, y_pred)\n",
        "    report = classification_report(y_test_encoded_np, y_pred)\n",
        "    cm = confusion_matrix(y_test_encoded_np, y_pred)\n",
        "\n",
        "    results[name] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"report\": report,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"predictions\": y_pred\n",
        "    }\n",
        "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Print results"
      ],
      "metadata": {
        "id": "uqB_6noljoCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print detailed results and plot confusion matrices\n",
        "for name, res in results.items():\n",
        "    print(f\"--- Results for {name} ---\")\n",
        "    print(f\"Accuracy: {res['accuracy']:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(res['report'])\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 40 + \"\\n\")"
      ],
      "metadata": {
        "id": "_Q1D88xqKLT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. Compute metrics and plot"
      ],
      "metadata": {
        "id": "dDXufUNnjtmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute verification metrics\n",
        "genuine_scores, impostor_scores = compute_verification_metrics(\n",
        "    X_test,\n",
        "    y_test_encoded,\n",
        "    num_pairs=NUM_PAIRS\n",
        ")\n",
        "\n",
        "# Plot verification metrics\n",
        "eer, eer_threshold = plot_verification_metrics(genuine_scores, impostor_scores)\n",
        "print(f\"Equal Error Rate (EER): {eer:.4f}\")\n",
        "print(f\"Optimal Threshold: {eer_threshold:.4f}\")"
      ],
      "metadata": {
        "id": "kgkQuck9Mhd6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1F6Oy_HtwewXrUfnEdpeQ6PoHOY0sqlQd",
      "authorship_tag": "ABX9TyNWPRyiALjAedfRwVmDKhiq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}